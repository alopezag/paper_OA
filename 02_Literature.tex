\section{Related works}\label{sec:Literature}

The application of DAs within manufacturing environments has started being reported in recent literature \cite{rabelo2019CollaborativeSoftbotsEnhancing,traub2018FrameworkIllustratingDecisionmaking}. In 2019, Gartner predicted that by the end of this year over 25\% of so-called digital workers will make daily use of a virtual assistant of some sort \cite{gartner2019GartnerPredicts25}. Despite encouraging first steps, with positive POC results \cite{bosch2020SpatialAugmentedReality}, a number of characteristics specific to assembly environments lead to unresolved challenges that inhibit a wide rollout of DAs in these environments. Take a simple example where we consider a scenario where the DA wants to support the operator in his next task. We want the DA to answer the question of what the next task is as well as to proactively answer the question of the operator: "Why do I have an issue with executing this task?" Thinking through this example, the following challenges surface.

In order for the DA to determine the next task, the DA should understand what is going on. Awareness on the current state of the assembly process to deduce what potential follow-up actions could be. Because of the size and dynamics of the assembly environment and the freedoms the operators often have, capturing this assembly state is challenging. The DA will also require two-way interaction with the operator to gain further contextual information and to suggest support actions. Due to the heterogeneity of working environments, it is impossible to define one generic User Interface (UI) to provide that seamless interaction. E.g., in noisy environments, a touch-based interface is preferred over interaction through speech, while in other settings, the higher information throughput of speech might be preferred. Also, tasks that require the use of both hands and touch-based interaction through tablets are impossible, while this might be ideal in circumstances in which the operator needs to be mobile.

Lastly, the DA requires intelligence to provide answers. This encompasses two aspects: first, a deep and clear understanding and knowledge of the assembly process and environment. This knowledge can only be obtained through the integration of and information sharing between various, currently unlinked, and unstructured information sources. Second, the DA requires the intelligence to reason on this knowledge and link it to the perceived context to formulate answers to understand the why question and provide answers. DA solutions in other domains are based on data-driven methods that rely on vast (crowd-sourced) training datasets to provide this kind of reasoning intelligence and knowledge to the system. In assembly environments, especially in the case of high variability and low volume, these datasets are not always available because of the low occurrence of some tasks.

%ASE
To enable proactive contextualized support to the operator, the DA requires accurate information about the actual status of completion (state) of the product assembly and the actions that are currently being performed (i.e., the operator performs an event at a certain location: pick/place object, welding, gluing). In other words, actions are the transitions between two assembly states.
 
Vision-based action recognition \cite{beddiar2020VisionbasedHumanActivity, zhang2017ReviewHumanActivity} is an active domain of research. Various image-features can be used for action classification using template matching, generative models (Hidden Markov Models), or discriminative models (Support Vector Machines). Researchers in \cite{kaczmarek2015ProgressMonitoringGesture,karcher2018SensordrivenAnalysisManual} performed this type of research from an industrial point of view, but their methods are respectively limited to tracking operators' hands in assembly regions or tracking parts and tools with a variety of sensors (RFID, accelerometers). Consequently, these methods cannot be conclusive about performed assembly actions, nor the actual assembly state.

Various commercial, mainly vision-based, platforms exist that already capture assembly state information to a certain extent. Examples are AR-based operator monitoring and guidance systems (ARKITE, Light Guide Systems), which recognize pick-and-place actions of parts and tools based on the work station's layout, and static DWI platforms with quality monitoring functionalities (Ansomat, SmartKlaus). Within FAMAR ICON project \cite{zogopoulos2021ImagebasedStateTracking}, a state estimation solution was developed based on tool, hand and part tracking (Figure 3, light grey blocks). FAMAR's solution improves the capability of commercial platforms by providing insights into the actions performed in a specific region of interest (e.g., Did the operator insert the part in the right location?). However, there are limitations regarding (i) covering complete workstations and (ii) handling variable sequences and working methods. 
